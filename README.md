# Kokomi-LLaMA3
从0到1的大规模预训练语言模型开发与优化

项目描述：
本项目从零构建并优化了一个 千万参数 的大规模预训练语言模型，涵盖了预训练（Pretrain）、有监督微调（SFT）以及推理蒸馏（R1）三个关键阶段，最终打造了一个基于最小可用参数、能够流畅执行问答任务且具备思维链推理能力的大模型基座。
专业能力与技术亮点：
- 模型架构设计与实现：
在模型结构设计上，从零实现类LLaMA3的前沿架构，结合RMSNorm、分组注意力机制、SwiGLU激活层和RoPE位置编码，突破了传统的神经网络结构，使模型能够高效地处理复杂任务和长文本。同时，设计并实现了前向传播和生成采样策略，确保了模型在多任务处理中的稳定性和性能。
- Tokenizer与多语言支持：
负责从零设计并实现了基于BBPE编码的分词器，获得良好中文支持能力，同时保障了高效的词表压缩率与解码能力。通过设计轻量级词表，在确保模型处理能力的同时，有效提升了模型的多语言处理能力，为后续任务的多语言适配奠定了坚实的基础。
- 从头构建预训练流程：
在预训练阶段，从零实现了整个训练流程，包括手动设计token级别的损失函数、前向传播计算以及梯度累积与更新过程。利用自动化混合精度训练技术，极大提升了训练过程中的计算效率和训练稳定性，在大规模数据集上实现了高效的预训练。
- 有监督微调（SFT）：
在有监督微调阶段，基于匠数大模型开源SFT数据集进行数据格式化与优化，完成了指令微调，显著提升了模型在指令理解与任务执行中的表现。工程上，手动实现了SFT阶段的损失计算，如对指令部分的损失掩码，以更好地指导模型的微调过程。
- R1推理蒸馏与优化：
在R1推理蒸馏阶段，通过Deepseek-R1蒸馏方法对大模型进行了进一步的优化，使其能够具备慢思考能力，显著提高了模型在复杂推理任务中的表现。工程上，清洗和改造蒸馏数据，复用SFT部分的训练代码进行黑盒蒸馏，并根据蒸馏结果调整损失权重，以优化模型的推理和思维能力。

运行本项目需自行导入数据集:
  pretrain阶段使用的数据集需命名为：pretrain.jsonl(该数据集也是训练分词器所用的数据集)
  SFT阶段数据集名称：SFT.jsonl
  SFT长文本强化训练数据集名称：SFT_1024.jsonl
  R1推理蒸馏推荐使用与DeepSeek—R1类似带有思维链的数据集：r1_1024.jsonl
    例如如下数据：
    {"conversations":
    [{"role": "user",
"content": "请帮我写一个古风诗句，表达一种淡淡的哀愁和思念。"}, 
 {"role": "assistant", 
"content": "<think>\n好吧，用户让我帮他写一个古风诗句，主题是淡淡的哀愁和思念。首先，我需要理解他的需求。他可能希望这句诗能传达一种难以言喻的悲伤，但又不失古风的意境，这可能用于文学创作或者表达情感。\n\n我应该考虑古风诗的结构和常用词汇。古风诗常常使用自然意象，比如山水、 Places、四季等，来营造氛围。哀愁和思念可能涉及离别、思念、痛苦等情感。\n\n接着，我需要选择合适的词语来表达这些情感。比如“无尽有”可以表达无法言喻的悲伤，“空字”作为思念的代名词，这样既简洁又富有情感。如果季节有变化，换个季节会更有变化。\n\n然后是描绘场景，比如“天涯冷月”强调southenly的寒冷，增添夜晚的氛围，增加诗意。再加“孤影凄凄照”来表达内心的思念，最后一句“苦耐心犹未去”直接点明哀愁。\n\n最后，检查句子的对仗和节奏，确保流畅自然，符合古风的韵律。这样组合起来，应该能够满足用户的需求。\n</think>\n
<answer>\n无尽有，空字，若无云处。天涯冷月，孤影凄凄照，苦耐心犹未去。\n</answer>"}]}
